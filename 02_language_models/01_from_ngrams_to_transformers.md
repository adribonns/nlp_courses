# ðŸ§  1. From n-grams to Transformers

Language Modeling (LM) is the task of assigning a probability to a sequence of words.
It forms the mathematical foundation of most NLP systems â€” from predictive text to GPT-style models.

---

## 1.1 What Is a Language Model?

A language model estimates the probability of a word sequence:

P(wâ‚, wâ‚‚, ..., wâ‚™)

By the chain rule of probability:

P(wâ‚, wâ‚‚, ..., wâ‚™) = Î  (from i=1 to n) P(wáµ¢ | wâ‚, ..., wáµ¢â‚‹â‚)

This is the **generative view of language**: predicting the next word given the previous context.

---

## 1.2 The n-gram Model

Early language models assumed that only the last nâˆ’1 words matter (Markov assumption):

P(wáµ¢ | wâ‚, ..., wáµ¢â‚‹â‚) â‰ˆ P(wáµ¢ | wáµ¢â‚‹â‚â‚™â‚‹â‚â‚Ž, ..., wáµ¢â‚‹â‚)

### Example (bigram model)
P("the cat sat") = P("the") Ã— P("cat" | "the") Ã— P("sat" | "cat")

### Key idea
- Build frequency counts from a corpus.  
- Estimate probabilities using relative frequencies:
  P(wáµ¢ | wáµ¢â‚‹â‚) = count(wáµ¢â‚‹â‚, wáµ¢) / count(wáµ¢â‚‹â‚)

---

## 1.3 The Problem of Sparsity

Language is vast â€” many word sequences never appear in training data.

To deal with unseen n-grams, **smoothing** techniques are used:
- Additive (Laplace) smoothing
- Good-Turing discounting
- Kneser-Ney smoothing (state-of-the-art for n-grams)

Even so, n-gram models:
- Need huge storage for large n
- Struggle with long-range dependencies
- Treat words as discrete symbols (no notion of meaning)

---

## 1.4 The Neural Language Model (Bengio et al., 2003)

Introduced the idea of **learning distributed word representations** and predicting with a neural network.

Instead of discrete symbols, each word wáµ¢ is mapped to a continuous vector e(wáµ¢).  
These vectors capture **semantic similarity**.

Network architecture (simplified):

Input: [e(wáµ¢â‚‹â‚ƒ), e(wáµ¢â‚‹â‚‚), e(wáµ¢â‚‹â‚)]  
â†’ Hidden layer â†’ Softmax output over vocabulary

### Advantages
- Generalizes to unseen n-grams via embeddings  
- Learns smooth probability distributions  
- First step toward **deep learning in NLP**

**Reference:** Bengio, Y. et al. (2003). *A Neural Probabilistic Language Model.*

---

## 1.5 Recurrent Neural Networks (RNNs)

To model longer dependencies, we need recurrence:

At each time step t:
- hâ‚œ = f(Wâ‚“xâ‚œ + Wâ‚•hâ‚œâ‚‹â‚)
- yâ‚œ = softmax(Wâ‚’hâ‚œ)

Here, hâ‚œ is a **hidden state** summarizing all previous context.

**Strengths**
- Can, in theory, capture unbounded context.
- Works well for sequential data (text, speech).

**Weaknesses**
- Hard to train on long sequences (vanishing gradients).
- Sequential computation prevents parallelization.

---

## 1.6 LSTM and GRU: Gated RNNs

To mitigate vanishing gradients, gated architectures were proposed:

### LSTM (Hochreiter & Schmidhuber, 1997)
Adds memory cells and gates:
- Input gate: controls how much new info enters.
- Forget gate: controls what to discard.
- Output gate: controls what to emit.

This allows **long-term dependency tracking**.

### GRU (Cho et al., 2014)
Simpler alternative: merges input and forget gates.

---

## 1.7 The Rise of Attention

RNNs process tokens sequentially â€” slow and limited.  
In 2014, Bahdanau et al. introduced **attention** for machine translation.

Key idea: at each step, the model â€œattendsâ€ to relevant parts of the input sequence.

Attention weight for token i at step t:
Î±â‚œáµ¢ = softmax(score(hâ‚œâ‚‹â‚, háµ¢))

Context vector:
câ‚œ = Î£ Î±â‚œáµ¢ Â· háµ¢

This allows dynamic focusing on different parts of the sequence â€” a major breakthrough.

---

## 1.8 Transformers (Vaswani et al., 2017)

Transformers replaced recurrence entirely with **self-attention**.

Each token attends to all others in parallel, using the **Scaled Dot-Product Attention** mechanism.

For a query Q, key K, and value V:

Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) V

Where dâ‚– is the key dimension.

### Multi-head Attention
Multiple attention â€œheadsâ€ run in parallel, allowing the model to capture different types of relationships.

Output = Concat(headâ‚, ..., headâ‚•) Ã— Wâ‚’

---

## 1.9 Advantages of Transformers

- Fully parallelizable (fast training)
- Capture long-range dependencies efficiently
- Scale well with data and parameters
- Foundation for large pre-trained models (BERT, GPT, T5)

---

## 1.10 Historical Summary

| Era | Model | Key Innovation |
|:--|:--|:--|
| 1980sâ€“1990s | n-gram | Statistical co-occurrence |
| 2003 | Neural LM | Embeddings + NN prediction |
| 2013â€“2015 | RNN/LSTM/GRU | Sequential modeling |
| 2014 | Attention | Dynamic context weighting |
| 2017 | Transformer | Parallel self-attention |

---

## 1.11 References

- Bengio, Y. et al. (2003). *A Neural Probabilistic Language Model.*  
- Bahdanau, D. et al. (2014). *Neural Machine Translation by Jointly Learning to Align and Translate.*  
- Vaswani, A. et al. (2017). *Attention is All You Need.*  
- Hochreiter, S. & Schmidhuber, J. (1997). *Long Short-Term Memory.*

---

## âœ… Summary

> The path from n-grams to transformers represents the evolution of NLP itself:  
> from discrete counts to continuous representations, from local to global context, and from statistical models to massive pretrained architectures.  
> Understanding this journey is key to grasping how modern language models like GPT actually work.


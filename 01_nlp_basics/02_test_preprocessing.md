# ğŸ”¤ 2. Text Preprocessing

Before any model can understand or generate text, raw language must be **cleaned, structured, and normalized**.  
Text preprocessing transforms unstructured text into a form suitable for computational analysis.

---

## 2.1 Why Preprocessing Matters

Raw text contains noise: punctuation, capitalization, typos, and linguistic variations.  
Effective preprocessing improves both the **accuracy** and **efficiency** of NLP models.

Typical goals:
- Standardize input  
- Reduce vocabulary size  
- Improve feature extraction consistency

---

## 2.2 NLP Preprocessing Pipeline (Conceptual Diagram)

â†’ Tokenization
â†’ Normalization
â†’ Stopword Removal
â†’ Stemming / Lemmatization
â†’ Vectorization


Each step refines the representation of text to help algorithms capture meaning more effectively.

---

## 2.3 Tokenization

**Definition:** Splitting text into meaningful units called *tokens* (words, subwords, or characters).

### Types of tokenization
| Type | Example Input | Example Tokens |
|:--|:--|:--|
| **Word-level** | â€œNatural Language Processingâ€ | ["Natural", "Language", "Processing"] |
| **Subword-level** | â€œunhappinessâ€ | ["un", "##happiness"] |
| **Character-level** | â€œNLPâ€ | ["N", "L", "P"] |
| **Sentence-level** | â€œHello world. How are you?â€ | ["Hello world.", "How are you?"] |

### Notes
- Tokenization is **language-dependent**.  
- Modern LLMs (e.g., GPT, BERT) use **subword tokenization** like BPE (Byte-Pair Encoding) or WordPiece to handle rare words and balance vocabulary size.

**Formula (approximate vocabulary compression):**
$$V_{new} = V_{old} - n + 1$$
where \(n\) is the number of symbol pairs merged during BPE training.

**Key references:**  
- Sennrich et al. (2016) â€” *Neural Machine Translation of Rare Words with Subword Units.*

---

## 2.4 Normalization

Normalization reduces variation in the text without altering meaning.

### Common steps:
- Lowercasing: â€œAppleâ€ â†’ â€œappleâ€  
- Removing punctuation and special characters  
- Handling numbers: â€œ2025â€ â†’ â€œ<NUM>â€  
- Unicode normalization (e.g., removing diacritics: â€œcafÃ©â€ â†’ â€œcafeâ€)  
- Expanding contractions: â€œdonâ€™tâ€ â†’ â€œdo notâ€

> âš ï¸ Be careful: excessive normalization can remove meaningful distinctions (e.g., â€œUSâ€ vs â€œusâ€).

---

## 2.5 Stopword Removal

Stopwords are common words (e.g., *the, is, of, and*) that usually carry limited semantic content.

- Common libraries: `NLTK`, `spaCy`, or custom domain-specific lists.  
- However, **contextual models** like BERT or GPT no longer need explicit stopword removal â€” they learn importance implicitly.

---

## 2.6 Stemming and Lemmatization

### ğŸ”¹ **Stemming**
Heuristic process that chops off word endings to reduce them to their base form.  
Example:  
`running â†’ run`, `flies â†’ fli`

**Algorithms:** Porter Stemmer, Snowball Stemmer.

Stemming formula (simplified):
$$f_{stem}(word) = prefix(word, k)$$
where \(k\) truncates suffixes according to rule-based heuristics.

---

### ğŸ”¹ **Lemmatization**
Linguistic process using vocabulary and morphology to find the canonical form (*lemma*).

Example:  
`am, are, is â†’ be`  
`better â†’ good`

Lemmatization considers:
- Part-of-speech tagging  
- Morphological analysis  

**Tools:** WordNetLemmatizer (`NLTK`), spaCy lemmatizer.

---

## 2.7 Handling Noise and Special Tokens

| Issue | Example | Typical Solution |
|:--|:--|:--|
| URLs | `https://openai.com` | Replace with `<URL>` |
| Numbers | `123` | `<NUM>` |
| Emojis | ğŸ™‚ ğŸ˜¢ | Map to sentiment or remove |
| Typos | â€œgooodâ€ | Spell correction models |
| Repeated chars | â€œsooooâ€ | Normalization rules |

---

## 2.8 Advanced Text Cleaning

- **Named Entity Replacement:** Replace entities with placeholders â†’ `John works at Google.` â†’ `<PERSON> works at <ORG>.`  
- **Handling code-mixed text:** Separate tokens from multiple languages.  
- **Language detection:** Detect and normalize multilingual inputs.

---

## 2.9 Summary Diagram (Textually Described)

1. Input: â€œThe cats are running faster than the dogs!â€  
2. Tokenization â†’ ["The", "cats", "are", "running", "faster", "than", "the", "dogs", "!"]  
3. Normalization â†’ ["cats", "running", "faster", "dogs"]  
4. Lemmatization â†’ ["cat", "run", "fast", "dog"]

---

## 2.10 Key References

- Porter, M. F. (1980). *An algorithm for suffix stripping.*  
- Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units.*  
- Manning, C. D., Raghavan, P., & SchÃ¼tze, H. (2008). *Introduction to Information Retrieval.*

---

## âœ… Summary

> Preprocessing transforms raw linguistic data into structured, analyzable input.  
> While early NLP relied heavily on manual cleaning, modern neural models integrate tokenization and normalization internally â€” yet understanding these steps remains essential for data quality and interpretability.

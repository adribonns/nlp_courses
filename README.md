# 🧠 NLP & Language Models — From Fundamentals to Retrieval-Augmented Generation

Welcome to this comprehensive and structured course on **Natural Language Processing (NLP)**, **Language Models (LMs)**, and **Retrieval-Augmented Generation (RAG)**.  
This repository provides a **concise yet complete theoretical foundation** for understanding modern NLP and large language models — with a focus on clarity, intuition, and references to key works.

---

## 🎯 Objectives

- Understand the evolution of NLP, from symbolic methods to neural architectures.  
- Master the theoretical principles behind modern **language models** and **transformer-based architectures**.  
- Grasp the core ideas and challenges of **RAG (Retrieval-Augmented Generation)** systems.  
- Build a strong conceptual foundation for applied research or system design.

---

## 📚 Course Structure

| Module | Folder | Topics |
|:--|:--|:--|
| **1. NLP Fundamentals** | [`01_nlp_basics/`](./01_nlp_basics/) | Text preprocessing, feature extraction, evaluation |
| **2. Language Models** | [`02_language_models/`](./02_language_models/) | From n-grams to transformers, embeddings, fine-tuning |
| **3. RAG Systems** | [`03_rag/`](./03_rag/) | Retrieval, generation, architectures, evaluation |

---

## 🧩 What’s Inside

Each section is designed as a short course module with:
- **Concept explanations** and **mathematical foundations**  
- **Conceptual diagrams** (described textually)  
- **Historical context** and **key references**  
- **Connections between classical and modern methods**

> ⚠️ This course is mainly theoretical and does not include executable code.  
> When useful, we mention tools such as `Hugging Face`, `LangChain`, or `ChromaDB` for context.

---

## 📖 Recommended Background

- Basic knowledge of **machine learning** (loss, optimization, overfitting).  
- Some familiarity with **Python** and **probability**.  
- No deep math background required — intuition is prioritized.

---

## 🧠 Further Reading & References

- Jurafsky & Martin — *Speech and Language Processing* (3rd ed. draft)  
- Goldberg, Y. — *Neural Network Methods for NLP*  
- Vaswani et al. (2017) — *Attention Is All You Need*  
- Lewis et al. (2020) — *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*  
- Bommasani et al. (2021) — *On the Opportunities and Risks of Foundation Models*

---

## 🧩 License & Contributions

This content is released under the **Creative Commons Attribution (CC BY 4.0)** license.  
You are free to reuse, adapt, and share with attribution.

Contributions and corrections are welcome — feel free to open issues or pull requests.

---

**Author:** *Adrien BONNEU*  
**Created:** 2025  
**Version:** 1.0

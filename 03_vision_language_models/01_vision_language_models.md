# üñºÔ∏è 1. Vision‚ÄìLanguage Models (VLMs)

As language models evolve, they increasingly need to understand the *world* ‚Äî not just text.  
Vision‚ÄìLanguage Models (VLMs) extend LLMs to perceive and reason over **images, video, or other modalities**.

---

## 1.1 Motivation

Natural language alone cannot express everything we perceive.  
Tasks such as:
- describing an image (‚ÄúWhat‚Äôs happening here?‚Äù),
- answering visual questions (‚ÄúHow many people are in the photo?‚Äù),
- reasoning about scenes (‚ÄúWhy is the man holding an umbrella?‚Äù)

require **joint understanding of text and vision**.

Modern VLMs aim to **align visual and textual representations** into a shared semantic space.

---

## 1.2 Architecture Overview

VLMs combine two major components:

[ Image Encoder ] + [ Text Encoder / Decoder ]
‚Üì ‚Üì
Vision features Text embeddings
‚Üì
Multimodal fusion
‚Üì
Output (caption, answer, etc.


The **fusion layer** is what makes the model multimodal ‚Äî it connects the two streams into a coherent joint representation.

---

## 1.3 Vision Modules

### 1.3.1 Convolutional Networks (CNNs)
Early models (e.g., *Show and Tell*, 2015) used CNNs such as ResNet to extract features:
$$
v = f_{\text{CNN}}(I) \in \mathbb{R}^{d_v}
$$
where $I$ is the image, and $v$ its feature vector.

### 1.3.2 Vision Transformers (ViT)
Transformers applied directly to image patches (Dosovitskiy et al., 2020):

1. Split image into $N$ patches.
2. Flatten and embed each patch:
   $$
   x_i = W_E \cdot \text{Flatten}(p_i)
   $$
3. Add positional embeddings and feed through Transformer encoder.

Output is a set of contextualized patch tokens ‚Äî analog to text tokens.

---

## 1.4 Text Modules

Usually pretrained on massive corpora (BERT, T5, GPT, LLaMA).  
They produce embeddings or generate text sequences.

### Encoder-based (e.g., BERT)
For understanding tasks (retrieval, VQA, grounding).

### Decoder-based (e.g., GPT)
For generative tasks (captioning, dialogue, visual reasoning).

---

## 1.5 Fusion Mechanisms

How vision and language features are combined determines the model class.

| Type | Fusion mechanism | Example models |
|:--|:--|:--|
| **Early Fusion** | Combine visual + text embeddings before Transformer | VisualBERT, ViLBERT |
| **Late Fusion** | Separate encoders, alignment in embedding space | CLIP, ALIGN |
| **Cross-Attention Fusion** | Vision features feed into LM via attention layers | BLIP-2, Flamingo, LLaVA |
| **Projection + Prompting** | Image features projected into token embeddings | GPT-4V, Kosmos-2 |

---

## 1.6 Training Objectives

### 1.6.1 Contrastive Learning (CLIP-style)

Align image‚Äìtext pairs in a joint embedding space.

Given image $I$ and caption $T$, with encoders $f_v$ and $f_t$:

$$
\mathcal{L}_{\text{contrastive}} = - \log \frac{\exp(\text{sim}(f_v(I), f_t(T))/\tau)}{\sum_{T'} \exp(\text{sim}(f_v(I), f_t(T'))/\tau)}
$$

where `sim` is cosine similarity, and $\tau$ is a temperature parameter.

**Effect:**  
Image and text representations are close if they match semantically.

**Models:** CLIP (OpenAI, 2021), ALIGN (Google).

---

### 1.6.2 Generative Objectives

Models like BLIP and Flamingo treat the image as *context* for text generation:

$$
\mathcal{L}_{\text{gen}} = - \sum_t \log P_\theta(y_t | y_{<t}, v)
$$

This allows captioning, reasoning, and multimodal QA.

**Models:** BLIP, BLIP-2, Flamingo, GPT-4V.

---

### 1.6.3 Multi-task Training

Many VLMs combine multiple objectives:
- image‚Äìtext contrastive
- image‚Äìtext matching (binary classification)
- text generation (captioning)
- masked modeling (for robustness)

---

## 1.7 Representative Architectures

| Model | Year | Type | Core Idea |
|:--|:--|:--|:--|
| **CLIP** (OpenAI) | 2021 | Contrastive | Align images and text via joint embedding |
| **ALIGN** (Google) | 2021 | Contrastive | Scale up CLIP to billions of pairs |
| **BLIP / BLIP-2** (Salesforce) | 2022‚Äì23 | Vision‚Äìlanguage pretraining | Bridge pretrained encoders and decoders via Q-former |
| **Flamingo** (DeepMind) | 2022 | Cross-attention fusion | Inject visual features into frozen LLM |
| **Kosmos-2** (Microsoft) | 2023 | Multimodal LLM | Integrate perception + grounding |
| **LLaVA** (Vicuna + CLIP) | 2023 | Instruction-tuned | Use image features as input tokens to LLM |
| **GPT-4V / Gemini / Claude 3 Opus** | 2023‚Äì24 | Multimodal reasoning | Unified vision‚Äìlanguage‚Äìtool reasoning |

---

## 1.8 The ‚ÄúQ-former‚Äù Bridge (BLIP-2)

A key design in BLIP-2 is the **Q-former**, a lightweight transformer that maps visual tokens into a query embedding space understandable by an LLM.

### Mechanism
1. Visual encoder ‚Üí image tokens $v_i$
2. Learnable queries $Q$ attend to $v_i$ via cross-attention
3. Produce compact representation $Q'$ fed into text model

This allows coupling **frozen pretrained components** (ViT + LLM) with minimal fine-tuning.

---

## 1.9 Visual Instruction Tuning

To make VLMs follow natural-language instructions, they are fine-tuned on datasets of (image, instruction, response) triplets.

Example (LLaVA-style):
User: What is the animal doing?
[Image of a cat sleeping]
Assistant: The cat is sleeping on a blanket.


This creates **multimodal chatbots** capable of grounded reasoning and vision-language alignment.

---

## 1.10 Evaluation and Benchmarks

| Task | Dataset | Metric |
|:--|:--|:--|
| **Image Captioning** | COCO, Flickr30k | BLEU, CIDEr |
| **Visual QA** | VQA-v2, GQA | Accuracy |
| **Visual Reasoning** | ScienceQA, VizWiz | Exact match |
| **Retrieval** | ImageNet, MS-COCO | Recall@K |
| **Multimodal Chat** | MM-Bench, LLaVA-Bench | Human eval, GPT-based scores |

---

## 1.11 Beyond Images: Toward Multimodality

Modern models extend to:
- **Video‚Äìtext** (VideoCLIP, Flamingo-2)
- **Audio‚Äìtext** (Whisper, SpeechT5)
- **3D grounding** (Point-BERT, Objaverse)
- **Tool and sensor inputs** (robotics, embodied AI)

**Goal:** create *foundation models* capable of understanding the world in all modalities.

---

## 1.12 Conceptual Diagram (Textual)

      +----------------------+
      |  Vision Encoder (ViT)|
      +----------------------+
                   ‚Üì
         [Visual embeddings]
                   ‚Üì
      +----------------------+
      |  Fusion / Projection |
      +----------------------+
                   ‚Üì
      +----------------------+
      |  Language Model (LLM)|
      +----------------------+
                   ‚Üì
          [Caption / Answer / Reasoning]

---

## üßæ References

- Radford et al. (2021). *Learning Transferable Visual Models From Natural Language Supervision (CLIP).*  
- Li et al. (2022). *BLIP: Bootstrapped Language‚ÄìImage Pretraining.*  
- Li et al. (2023). *BLIP-2: Bootstrapping Language‚ÄìImage Pretraining with Frozen Image Encoders and Large Language Models.*  
- Alayrac et al. (2022). *Flamingo: Visual Language Models with Few-Shot Learning.*  
- Liu et al. (2023). *Visual Instruction Tuning (LLaVA).*  
- OpenAI (2023). *GPT-4 Technical Report.*

---

## ‚úÖ Summary

> Vision‚ÄìLanguage Models extend language models with perception.  
> They combine vision encoders (CNN, ViT) with text encoders or decoders, using contrastive or generative objectives to align the two modalities.  
> Modern architectures like CLIP, BLIP-2, Flamingo, and GPT-4V enable multimodal reasoning, forming the basis of AI systems that *see, read, and understand*.
)

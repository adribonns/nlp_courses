# üß© 2. Multimodal Reasoning and Alignment

While early Vision‚ÄìLanguage Models (VLMs) aligned *representations* (images ‚Üî text),
modern systems like **GPT-4V**, **Gemini**, and **Claude 3 Opus** perform *reasoning* across modalities:
they can analyze, infer, explain, and generalize from multimodal inputs.

---

## 2.1 From Perception to Reasoning

We can view multimodal understanding as a hierarchy:

| Level | Description | Example |
|:--|:--|:--|
| **Perception** | Recognize objects, scenes, and text | ‚ÄúA cat on a table.‚Äù |
| **Understanding** | Describe relationships and actions | ‚ÄúThe cat is sitting on the table.‚Äù |
| **Reasoning** | Infer causes, predict outcomes, justify | ‚ÄúThe cat is probably waiting for food.‚Äù |

A capable VLM integrates **perceptual grounding** (vision) with **symbolic reasoning** (language).

---

## 2.2 Multimodal Alignment

**Goal:** ensure that visual and textual embeddings refer to the *same underlying meaning.*

### 2.2.1 Representation Alignment

For each image $I$ and caption $T$:

$$
\text{Align}(f_v(I)) \approx f_t(T)
$$

Measured by cosine similarity or contrastive loss (CLIP-style).

### 2.2.2 Semantic Alignment

Goes beyond vector proximity:  
The model must *understand* that ‚Äúa dog chasing a ball‚Äù ‚âà ‚Äúan animal running after a toy.‚Äù

Requires compositional reasoning and fine-grained grounding.

---

## 2.3 Visual Grounding

Visual grounding links **words ‚Üî regions** of an image.

Given an image and sentence, the model predicts bounding boxes $b_i$ corresponding to entities $w_i$:

$$
\text{Grounding: } w_i \rightarrow b_i
$$

Used in tasks such as **referring expressions**, **object grounding**, and **scene understanding.**

---

## 2.4 Cross-modal Attention

A key component for reasoning:

$$
A = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

- $Q$: queries from text tokens  
- $K, V$: keys and values from vision embeddings  

Cross-attention allows the language model to selectively "look" at relevant parts of an image while generating or reasoning.

[Text Token] ‚Üí attends to ‚Üí [Visual Features]
"What is the man holding?" ‚Üí focuses on the "ball" region


---

## 2.5 Chain-of-Thought (CoT) in Multimodal Models

Modern VLMs (e.g., GPT-4V, Gemini 1.5) integrate **reasoning traces** with multimodal input:

Example:

Q: Why is the boy wearing a helmet?
Image: [boy on a skateboard]
Reasoning: He is likely skateboarding, which requires safety gear.
A: Because he is skateboarding.


Training involves supervising both *answers* and *rationales*, often using synthetic CoT datasets.

---

## 2.6 Training for Alignment and Reasoning

### 2.6.1 Stage 1 ‚Äì Perceptual Pretraining
- Contrastive / masked image modeling (e.g., CLIP)
- Learn visual grounding and feature extraction

### 2.6.2 Stage 2 ‚Äì Cross-modal Instruction Tuning
- Dataset of (image, instruction, response)
- Objective:
  $$
  \mathcal{L}_{\text{instr}} = -\sum_t \log P(y_t | y_{<t}, I, x)
  $$

### 2.6.3 Stage 3 ‚Äì Reinforcement or Preference Alignment
- Human or model feedback on outputs
- Reward encourages truthful, helpful, safe multimodal behavior

---

## 2.7 Alignment Strategies (Post-training)

Borrowed from LLM alignment, extended to VLMs:

| Strategy | Description | Used in |
|:--|:--|:--|
| **SFT** (Supervised Fine-Tuning) | Train on curated multimodal QA/caption pairs | LLaVA, BLIP-2 |
| **RLHF** | Optimize rewards from human preference | InstructBLIP, GPT-4V |
| **DPO** (Direct Preference Optimization) | Train directly on preferred responses using contrastive objective | Gemini, Claude 3 |
| **RLAIF** | Replace human feedback with model feedback (AI Feedback) | GPT-4-Turbo, Gemini 1.5 |

---

## 2.8 Reinforcement for Vision‚ÄìLanguage

### 2.8.1 PPO (Proximal Policy Optimization)

Used in RLHF for aligning model behavior.

Given reward $r$, model parameters $\theta$ are optimized to maximize:

$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t) \right]
$$

where $A_t$ is the advantage estimate.

In VLMs, reward can depend on:
- Answer correctness
- Relevance to image
- Safety / factuality

### 2.8.2 DPO (Direct Preference Optimization)

Simpler alternative ‚Äî no reinforcement loop:

Given two outputs $(y^+, y^-)$, with human preference for $y^+$:

$$
\mathcal{L}_{\text{DPO}} = - \log \sigma \left( \beta \log \frac{\pi_\theta(y^+|x)}{\pi_{\text{ref}}(y^+|x)} - \beta \log \frac{\pi_\theta(y^-|x)}{\pi_{\text{ref}}(y^-|x)} \right)
$$

Advantages:
- Stable and efficient
- Works well with multimodal data

---

## 2.9 Multimodal Benchmarks

| Benchmark | Focus | Metric |
|:--|:--|:--|
| **MMBench / MM-Vet** | General multimodal reasoning | GPT-based eval |
| **ScienceQA** | Scientific diagrams + text | Accuracy |
| **VizWiz** | Blind accessibility | Human eval |
| **LLaVA-Bench** | Instruction following | GPT eval |
| **VQA-v2** | Visual question answering | Accuracy |

---

## 2.10 Evaluation Challenges

- **Ambiguity**: multiple valid answers (‚Äúa man playing soccer‚Äù vs ‚Äúa football player‚Äù)  
- **Compositional reasoning**: complex visual relations are hard to generalize  
- **Hallucination**: model invents objects not present in the image  
- **Biases**: social and dataset biases carry over from vision and text pretraining  

---

## 2.11 Future Directions

1. **Video reasoning** ‚Äî temporal understanding, event prediction  
2. **3D spatial grounding** ‚Äî embodied perception (e.g., robotics)  
3. **Symbolic reasoning + perception** ‚Äî bridging neural and logical systems  
4. **Continual multimodal learning** ‚Äî adapting across tasks and domains  
5. **Tool-augmented VLMs** ‚Äî combining vision, text, and external APIs (OCR, web search, calculators)

---

## 2.12 Conceptual Schema

+------------------------+
| Visual Encoder (ViT) |
+------------------------+
‚Üì
[Image embeddings]
‚Üì
+------------------------+
| Cross-Attention / Q-Former |
+------------------------+
‚Üì
[Aligned multimodal tokens]
‚Üì
+------------------------+
| Language Model (LLM) |
+------------------------+
‚Üì
Reasoning + Generation


---

## 2.13 Summary

> Modern VLMs no longer just match images and text ‚Äî they *reason* across modalities.  
> Alignment involves both **representation** (feature space) and **behavioral** (output preference) layers.  
> Techniques like **cross-attention**, **instruction tuning**, and **DPO/RLHF alignment** enable models like GPT-4V and Gemini to understand, explain, and reason about the visual world.

---

## üìö References

- OpenAI (2023). *GPT-4 Technical Report.*  
- Alayrac et al. (2022). *Flamingo: Visual Language Models with Few-Shot Learning.*  
- Li et al. (2023). *BLIP-2: Bootstrapping Language‚ÄìImage Pretraining with Frozen Image Encoders and LLMs.*  
- Rafailov et al. (2023). *Direct Preference Optimization: Your Language Model is Secretly a Reward Model.*  
- Bai et al. (2022). *Training a Helpful and Harmless Assistant with RLHF.*  
- Liu et al. (2023). *LLaVA: Visual Instruction Tuning.*


